# -*- coding: utf-8 -*-
"""bench.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16dTaBcxGKDS1Lbl7YgS6CRNQ5xxY3Gxj
"""

# ADVANCED TIME SERIES FORECASTING PROJECT
# Attention LSTM vs SARIMAX Benchmark


#  Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.model_selection import TimeSeriesSplit

from statsmodels.tsa.statespace.sarimax import SARIMAX

import tensorflow as tf
from tensorflow.keras.layers import LSTM, Dense, Input, Attention
from tensorflow.keras.models import Model

#  Data Generation
np.random.seed(42)

N = 1000
time = np.arange(N)

# Explicit parameters (now quantitative)
trend_slope = 0.05
seasonal_amplitude = 10
seasonal_period = 24
noise_std = 2.0

trend = trend_slope * time
seasonality = seasonal_amplitude * np.sin(2 * np.pi * time / seasonal_period)
noise = np.random.normal(0, noise_std, N)

target = 50 + trend + seasonality + noise

data = pd.DataFrame({
    "time": time,
    "target": target,
    "feature_1": target + np.random.normal(0, 1, N),
    "feature_2": np.random.normal(20, 5, N),
    "feature_3": np.random.normal(0, 1, N),
    "feature_4": np.random.normal(100, 10, N)
})

data.to_csv("synthetic_timeseries.csv", index=False)
print(data.head())

#  Scaling
scaler = MinMaxScaler()
scaled = scaler.fit_transform(data)
scaled_df = pd.DataFrame(scaled, columns=data.columns)

#  Create Sequences
LOOKBACK = 20

def create_sequences(df, lookback):
    X, y = [], []
    for i in range(len(df) - lookback):
        X.append(df.iloc[i:i+lookback, :-1].values)
        y.append(df.iloc[i+lookback, -1])
    return np.array(X), np.array(y)

X, y = create_sequences(scaled_df, LOOKBACK)

#  Train/Test Split
split = int(0.8 * len(X))
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

#  Build Attention LSTM Model
def build_model(lstm1=64, lstm2=32, lr=0.001):
    input_layer = Input(shape=(LOOKBACK, 5))
    lstm_out = LSTM(lstm1, return_sequences=True)(input_layer)
    attention = Attention()([lstm_out, lstm_out])
    lstm_out2 = LSTM(lstm2)(attention)
    output = Dense(1)(lstm_out2)

    model = Model(inputs=input_layer, outputs=output)
    model.compile(optimizer=tf.keras.optimizers.Adam(lr),
                  loss='mse')
    return model

model = build_model()
model.summary()

# Train Model
history = model.fit(
    X_train, y_train,
    epochs=20,
    batch_size=32,
    validation_data=(X_test, y_test),
    verbose=1
)

#  Deep Learning Evaluation
pred_dl = model.predict(X_test)
mae_dl = mean_absolute_error(y_test, pred_dl)
rmse_dl = np.sqrt(mean_squared_error(y_test, pred_dl))

print("\nAttention LSTM Performance")
print("MAE:", mae_dl)
print("RMSE:", rmse_dl)

#  SARIMAX Baseline
train_target = data['target'][:split+LOOKBACK]
test_target = data['target'][split+LOOKBACK:]

sarimax = SARIMAX(train_target,
                  order=(2,1,2),
                  seasonal_order=(1,1,1,12))
sarimax_model = sarimax.fit()

sarimax_pred = sarimax_model.forecast(len(test_target))

mae_s = mean_absolute_error(test_target, sarimax_pred)
rmse_s = np.sqrt(mean_squared_error(test_target, sarimax_pred))

print("\nSARIMAX Performance")
print("MAE:", mae_s)
print("RMSE:", rmse_s)

#  Rolling Cross Validation
tscv = TimeSeriesSplit(n_splits=5)
mae_scores, rmse_scores = [], []

for train_idx, val_idx in tscv.split(target):
    train, val = target[train_idx], target[val_idx]
    model = SARIMAX(train, order=(1,1,1), seasonal_order=(1,1,1,24))
    res = model.fit(disp=False)
    preds = res.forecast(len(val))

    mae_scores.append(mean_absolute_error(val, preds))
    rmse_scores.append(np.sqrt(mean_squared_error(val, preds)))

avg_mae = np.mean(mae_scores)
avg_rmse = np.mean(rmse_scores)

print("\nRolling Cross Validation Results")
print(f"Average MAE: {avg_mae:.2f}")
print(f"Average RMSE: {avg_rmse:.2f}")


# hyperparameter tuning

def build_model(lstm1=64, lstm2=32, lr=0.001):
    input_layer = Input(shape=(20, 5))
    lstm_out = LSTM(lstm1, return_sequences=True)(input_layer)

    attention = Attention()([lstm_out, lstm_out])
    lstm_out2 = LSTM(lstm2)(attention)

    output = Dense(1)(lstm_out2)

    model = Model(inputs=input_layer, outputs=output)
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),
                  loss='mse')
    return model

  # grid search

params = [
    (32, 16, 0.001),
    (64, 32, 0.001),
    (64, 32, 0.0005),
    (128, 64, 0.001),
]

results = []

for p in params:
    print("Testing:", p)
    model = build_model(*p)
    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)

    pred = model.predict(X_test)
    mae = mean_absolute_error(y_test, pred)
    rmse = np.sqrt(mean_squared_error(y_test, pred))

    results.append((p, mae, rmse))

results

#  Comparison Table
comparison = pd.DataFrame({
    "Model": ["Attention LSTM", "SARIMAX"],
    "MAE": [mae_dl, mae_s],
    "RMSE": [rmse_dl, rmse_s]
})

print("\nModel Comparison")
print(comparison)

#   Visualization
plt.figure(figsize=(12,6))
plt.plot(y_test[:200], label='Actual')
plt.plot(pred_dl[:200], label='Predicted')
plt.legend()
plt.title("Actual vs Predicted")
plt.show()

# attention weights
attention_layer = Model(inputs=model.input,
                        outputs=model.layers[2].output)

att_weights = attention_layer.predict(X_test[:1])

plt.imshow(att_weights[0], cmap='viridis')
plt.title("Attention Weights")
plt.colorbar()
plt.show()